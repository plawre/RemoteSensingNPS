{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0QOZbZ3hlb9B"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "import xarray as xr\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import SimpleRNN, Dense\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Define the base directory and file patterns\n",
        "base_dir = '/content/drive/MyDrive/hakData/'\n",
        "file_patterns = {\n",
        "    'nitrate': 'woa13_all_n01_01.nc',\n",
        "    'phosphate': 'woa13_all_p01_01.nc',\n",
        "    'silicate': 'woa13_all_i01_01.nc'\n",
        "}\n",
        "\n",
        "# Initialize dictionaries to hold the data\n",
        "lat = None\n",
        "lon = None\n",
        "nutrient_data = {}\n",
        "\n",
        "# Read the data for each nutrient\n",
        "for nutrient, pattern in file_patterns.items():\n",
        "    ds = xr.open_dataset(base_dir + pattern, engine='netcdf4', decode_times=False)\n",
        "\n",
        "    # Extract latitude and longitude (all are the same)\n",
        "    if lat is None and lon is None:\n",
        "        lat = ds['lat'].values\n",
        "        lon = ds['lon'].values\n",
        "\n",
        "    if nutrient == 'nitrate':\n",
        "        nutrient_data['nitrate'] = ds['n_an'].values[0, 0, :, :]  # Extract surface layer\n",
        "    elif nutrient == 'phosphate':\n",
        "        nutrient_data['phosphate'] = ds['p_an'].values[0, 0, :, :]  # Extract surface layer\n",
        "    elif nutrient == 'silicate':\n",
        "        nutrient_data['silicate'] = ds['i_an'].values[0, 0, :, :]  # Extract surface layer\n",
        "\n",
        "# Combine the nutrient data into a single 3x180x360 array\n",
        "combined_data = np.stack((nutrient_data['nitrate'], nutrient_data['phosphate'], nutrient_data['silicate']), axis=0)\n",
        "\n",
        "# Transpose combined_data to shape (180, 360, 3) to get (lat, lon, features)\n",
        "data = combined_data.transpose((1, 2, 0))\n",
        "\n",
        "# Separate inputs (phosphate, silicate) and output (nitrate)\n",
        "X = data[:, :, 1:]  # Input features: phosphate, silicate\n",
        "y = data[:, :, 0]   # Output: nitrate\n",
        "\n",
        "# Reshape X to have shape (180*360, 2) and y to (180*360, 1) for compatibility with the RNN\n",
        "X = X.reshape((180 * 360, 2))\n",
        "y = y.reshape((180 * 360, 1))\n",
        "\n",
        "# Combine X and y to filter out rows with nan values\n",
        "data_combined = np.hstack((X, y))\n",
        "\n",
        "# Remove rows with nan values\n",
        "data_combined = data_combined[~np.isnan(data_combined).any(axis=1)]\n",
        "\n",
        "# Separate the data back into X and y\n",
        "X = data_combined[:, :2]\n",
        "y = data_combined[:, 2:]\n",
        "\n",
        "# Add the dummy time dimension to X just make it simple like 1\n",
        "X = X.reshape((X.shape[0], 1, X.shape[1]))\n",
        "\n",
        "# Normalize the input features using StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X.reshape(-1, 2)).reshape(X.shape)\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Print the shapes to verify\n",
        "print(f'X_train shape: {X_train.shape}, y_train shape: {y_train.shape}')\n",
        "print(f'X_test shape: {X_test.shape}, y_test shape: {y_test.shape}')\n",
        "\n",
        "# Define model\n",
        "model = Sequential()\n",
        "model.add(SimpleRNN(50, activation='relu', input_shape=(1, 2)))\n",
        "model.add(Dense(1))\n",
        "\n",
        "# Compile the model with a low learning rate and using the keras optimizer and loss fx mse\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='mse')\n",
        "\n",
        "# Print the model summary\n",
        "model.summary()\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, epochs=50, batch_size=16, validation_data=(X_test, y_test))\n",
        "\n",
        "# Plot the training and validation loss\n",
        "plt.plot(history.history['loss'], label='train')\n",
        "plt.plot(history.history['val_loss'], label='test')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    }
  ]
}